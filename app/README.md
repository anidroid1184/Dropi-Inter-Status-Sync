# Sistema de Tracking Dropi-Inter

**VersiÃ³n 3.0.0 - Arquitectura de Microservicios**

Sistema automatizado para sincronizaciÃ³n de estados de paquetes entre Dropi e InterrapidÃ­simo, con **3 aplicaciones independientes y escalables**.

This project provides automated package status synchronization between Dropi and InterrapidÃ­simo systems, featuring **3 independent and scalable microservices**.

## ğŸš€ Nueva Arquitectura: 3 Apps Independientes

Este proyecto ha evolucionado de un monolito a **3 aplicaciones completamente independientes**:

### 1. **APP SCRAPER** - Scraping de Web
- ğŸ“‚ `app_scrapper/`
- ğŸ¯ Scraping sÃ­ncrono/asÃ­ncrono de InterrapidÃ­simo
- ğŸ“ Actualiza columna STATUS TRACKING en Google Sheets
- âš¡ Playwright para navegaciÃ³n automatizada

### 2. **APP COMPARER** - Comparador de Estados
- ğŸ“‚ `app_comparer/`
- ğŸ¯ Compara STATUS DROPI vs STATUS TRACKING
- ğŸ“ Calcula columnas COINCIDEN y ALERTA
- ğŸ§  NormalizaciÃ³n inteligente de estados

### 3. **APP MAKE DAILY REPORT** - Generador de Reportes
- ğŸ“‚ `app_make_dialy_report/`
- ğŸ¯ Genera reportes Excel con alertas
- ğŸ“ Sube reportes a Google Drive
- ğŸ“Š Formato profesional automÃ¡tico

## âœ¨ Ventajas de la Nueva Arquitectura

- **Independencia Total**: Cada app con su propio `venv`, `.env`, `credentials.json`
- **Escalabilidad**: Cada app puede desplegarse en diferentes servidores
- **Mantenibilidad**: Cambios en una app no afectan a las demÃ¡s
- **Testing Aislado**: Pruebas unitarias por app sin dependencias cruzadas
- **Deploy Independiente**: Actualiza solo la app que necesitas
- **Microservicios Ready**: Arquitectura lista para Docker/Kubernetes

## ğŸ“š DocumentaciÃ³n

### Apps Independientes
- ğŸ“˜ **`app_scrapper/README.md`** - DocumentaciÃ³n completa del Scraper
- ğŸ“— **`app_comparer/README.md`** - DocumentaciÃ³n completa del Comparador
- ğŸ“™ **`app_make_dialy_report/README.md`** - DocumentaciÃ³n completa del Reporter

### Sistema General (Legacy)
- Ver `docs/Overview.md`, `docs/Architecture.md`, `docs/Modules-and-APIs.md`, etc.

---

## ğŸš€ Inicio RÃ¡pido

### OpciÃ³n 1: Apps Independientes (Recomendado)

Cada app se instala y ejecuta por separado:

```bash
# 1. APP SCRAPER
cd app_scrapper
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# Editar .env y copiar credentials.json
python scraper_app.py --async

# 2. APP COMPARER
cd ../app_comparer
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# Editar .env y copiar credentials.json
python comparer_app.py

# 3. APP REPORTER
cd ../app_make_dialy_report
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# Editar .env y copiar credentials.json
python reporter_app.py --upload
```

### OpciÃ³n 2: Sistema Legacy (Monolito)

```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
python app.py
```

---

## ğŸ“ Estructura del Proyecto

```text
automatic/
â”œâ”€ app_scrapper/                   # ğŸ†• APP 1: Scraper independiente
â”‚  â”œâ”€ scraper_app.py               #    Entry point
â”‚  â”œâ”€ scraper_config.py            #    ConfiguraciÃ³n local
â”‚  â”œâ”€ scraper_web.py               #    Scraper sincrÃ³nico
â”‚  â”œâ”€ scraper_web_async.py         #    Scraper asÃ­ncrono
â”‚  â”œâ”€ requirements.txt             #    Dependencias
â”‚  â””â”€ README.md                    #    DocumentaciÃ³n
â”‚
â”œâ”€ app_comparer/                   # ğŸ†• APP 2: Comparador independiente
â”‚  â”œâ”€ comparer_app.py              #    Entry point
â”‚  â”œâ”€ comparer_normalizer.py       #    NormalizaciÃ³n de estados
â”‚  â”œâ”€ comparer_alerts.py           #    CÃ¡lculo de alertas
â”‚  â”œâ”€ dropi_map.json               #    Mapeo estados
â”‚  â”œâ”€ requirements.txt             #    Dependencias
â”‚  â””â”€ README.md                    #    DocumentaciÃ³n
â”‚
â”œâ”€ app_make_dialy_report/          # ğŸ†• APP 3: Reporter independiente
â”‚  â”œâ”€ reporter_app.py              #    Entry point
â”‚  â”œâ”€ reporter_excel.py            #    Generador Excel
â”‚  â”œâ”€ reporter_drive.py            #    Upload a Drive
â”‚  â”œâ”€ requirements.txt             #    Dependencias
â”‚  â””â”€ README.md                    #    DocumentaciÃ³n
â”‚
â”œâ”€ core/                           # Sistema Legacy
â”œâ”€ services/                       # Servicios Legacy
â”œâ”€ utils/                          # Utilidades Legacy
â”œâ”€ web/                            # Scrapers Legacy
â”œâ”€ scripts/                        # Scripts Legacy
â”œâ”€ docs/                           # DocumentaciÃ³n
â”œâ”€ deprecated/                     # Archivos obsoletos
â””â”€ README.md                       # Este archivo
```

### ğŸ†• Nuevos Componentes

- **MÃ³dulo `core/`**: LÃ³gica central de la aplicaciÃ³n modularizada
- **`status_normalizer.py`**: Normalizador especializado con soporte JSON
- **`alert_calculator.py`**: Calculador inteligente de alertas y reglas
- **`credentials_manager.py`**: GestiÃ³n segura y centralizada de credenciales
- **`batch_operations.py`**: Operaciones batch optimizadas para Google Sheets
- **`constants.py`**: Constantes del sistema centralizadas y tipadas

### ğŸ”„ Componentes Refactorizados

- **`tracker_service.py`**: Ahora actÃºa como fachada coordinadora
- **`app.py`**: FunciÃ³n main simplificada con manejo de errores robusto
- **MÃ³dulo `utils/`**: Expandido con nuevas utilidades especializadas

## Features / CaracterÃ­sticas

- Asynchronous scraping with Playwright + Chromium (concurrency, retries, RPS).
- Status normalization and alerting with configurable JSON mappings.
- Batch writes to Google Sheets to reduce API calls.
- Post-compare step to compute `COINCIDEN` and `ALERTA` over the whole sheet.
- Daily report generation that replaces the sheet every run.
- Detailed logging and per-tracking audit CSV.

## Requirements / Requisitos

- Python 3.10+
- `credentials.json` (Service Account) in the project root (not committed)

Install deps / Instalar dependencias:

```bash
pip install -r requirements.txt
# Install Playwright browser binaries (one-time)
python -m playwright install chromium
```

Ubuntu 25.04 note / Nota Ubuntu 25.04:

- Avoid running `playwright install-deps` on Ubuntu 25.04; dependency validation may fail or try to fetch incompatible packages.
- If Playwright refuses to run due to host requirement validation, set the environment variable to bypass validation:

```bash
export PLAYWRIGHT_SKIP_VALIDATE_HOST_REQUIREMENTS=1
```

<!-- Diagnostic virtual display section removed (Linux recreation not used in this repo) -->

## Environment / Entorno

Create a `.env` file (no secrets):

```ini
DRIVE_FOLDER_ID=...        # Google Drive folder ID with daily Excel
SPREADSHEET_NAME=seguimiento
HEADLESS=false             # false to see the browser while debugging; set true in production
TZ=America/Bogota
DAILY_REPORT_PREFIX=Informe_
```

## Quickstart (Windows, venv) / Inicio rÃ¡pido (Windows, venv)

Step-by-step with PowerShell / Paso a paso con PowerShell:

```powershell
# 1) Clone / Clonar
git clone <REPO_URL> automatic
cd automatic

# 2) Create and activate venv / Crear y activar entorno
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# 3) Install dependencies / Instalar dependencias
pip install --upgrade pip
pip install -r requirements.txt

# 4) Install Playwright browser / Instalar navegador de Playwright (Chromium)
python -m playwright install chromium

# 5) Prepare environment / Preparar entorno
# - Place credentials.json in project root / Poner credentials.json en la raÃ­z
# - Copy .env.example to .env and edit values / Copiar .env.example a .env y editar
Copy-Item .env.example .env -Force
# Edit .env to set HEADLESS=true/false, etc. / Editar .env para HEADLESS=true/false, etc.

# 6) Run (async, recommended) / Ejecutar (asÃ­ncrono, recomendado)
python app.py --async --start-row 2 --limit 50 --max-concurrency 3

# (Optional) Run sync / (Opcional) Ejecutar modo sÃ­ncrono
python app.py --start-row 2 --limit 50
```

Notes / Notas:

- Set `HEADLESS=false` in `.env` to see the browser while debugging locally; set `HEADLESS=true` for CI/servers.
- Ensure `credentials.json` corresponds to a Google Service Account with access to the target sheet and drive folder.

## Docker (optional) / Docker (opcional)

```bash
# Build (if needed)
docker build -t inter-tracker .
# Compose example (ensure volumes/env mapping for .env, credentials.json and logs/)
docker compose run --rm app bash -lc "python scripts/inter_process.py --start-row 2 --max-concurrency 2 --rps 0.8"
```

## Cron (1â€“2am COL)

Example (Linux cron) / Ejemplo (cron en Linux):

```bash
0 2 * * * /bin/bash -lc 'cd /path/automatic && source venv/bin/activate && python app.py --start-row 2 >> logs/$(date +\%F).log 2>&1'
```

## Notes / Notas

- `script.py` remains in place for now; new entrypoint is `app.py`.
- The scraper uses Playwright + Chromium; adjust headless with `HEADLESS`.
- Business rules and state normalization live in `services/tracker_service.py` and are driven by JSON mappings in project root.

## Async runner (recommended) / Runner asÃ­ncrono (recomendado)

Use the modular async runner to control concurrency, batches and rate limiting:

```bash
python scripts/inter_process.py \
  --start-row 2 \
  --batch-size 500 \
  --max-concurrency 3 \
  --rps 0.8 \
  --retries 2 \
  --timeout-ms 45000
```

App entrypoint (async flag) / Entrada principal (bandera asÃ­ncrona):

```bash
python app.py --async --start-row 2 --limit 50 --max-concurrency 3
```

Parameters / ParÃ¡metros:

- `--start-row`: primera fila (1-based) a procesar.
- `--end-row`: Ãºltima fila (opcional).
- `--limit`: mÃ¡ximo de filas a considerar (opcional).
- `--batch-size`: cantidad de guÃ­as por lote de navegador (default 500). Lotes grandes tardan en â€œarrancarâ€.
- `--max-concurrency`: pÃ¡ginas simultÃ¡neas (default 3). Impacta RAM/CPU.
- `--rps`: guÃ­as por segundo (espaciado de lanzamientos). Ãštil para evitar rÃ¡fagas.
- `--retries`: reintentos internos por guÃ­a si el estado llega vacÃ­o.
- `--timeout-ms`: timeout de navegaciÃ³n/esperas Playwright.
- `--batch-size`: tamaÃ±o de lote para el scraping asÃ­ncrono (filas por ciclo de navegador). Default 5000.
- `--post-compare`: despuÃ©s del scraping, ejecuta la comparaciÃ³n (COINCIDEN/ALERTA) antes del Daily Report.
- `--compare-batch-size`: tamaÃ±o de lote para la comparaciÃ³n. Default 5000.

- Seguro (producciÃ³n estable):
  - `--batch-size 500` `--max-concurrency 2` `--rps 0.6` `--retries 2` `--timeout-ms 60000`
- Moderado (cobertura rÃ¡pida):
  - `--batch-size 1000` `--max-concurrency 3` `--rps 0.8` `--retries 2` `--timeout-ms 45000`
- Visual (debug):
  - `--headless false` y lotes chicos `--limit 20 --batch-size 20`

Notas:

- `max-concurrency` consume RAM; con 4GB, 3 es tope razonable.
- `rps` controla rÃ¡fagas; 0.6â€“0.8 suele ser estable.

## Logging & observability / Observabilidad

- Logs diarios: `logs/YYYY-MM-DD.log`.
- Trazas Playwright con prefijo `[PW]` muestran: creaciÃ³n de contextos, pÃ¡ginas, navegaciÃ³n, popup y extracciÃ³n.
- Progreso por lote:
  - `Processing batch N with M items (rows X-Y)`
  - `Batch N progress: a/b (row idx, tn XXXXX)` cada 50 filas y al final del lote.
- AuditorÃ­a por tracking: `logs/statuses_YYYYMMDD.csv` (dropi/web raw y normalizado, alerta, vÃ­a).

## Examples / Ejemplos

ProducciÃ³n moderada (1000 por lote):

```bash
python scripts/inter_process.py --start-row 4000 --batch-size 1000 --max-concurrency 3 --rps 0.8 --retries 2 --timeout-ms 45000
```

Solo filas con `STATUS TRACKING` vacÃ­o:

```bash
python scripts/inter_process.py --start-row 4000 --batch-size 1000 --max-concurrency 3 --rps 0.8 --retries 2 --timeout-ms 45000 --only-empty
```

End-to-end (async app) / Fin-a-fin (app asÃ­ncrona):

Prueba acotada (sin Drive, rango pequeÃ±o, post-compare y daily report):

```bash
python app.py --async --skip-drive \
  --start-row 2 --end-row 200 \
  --max-concurrency 3 --batch-size 1000 \
  --post-compare --compare-batch-size 1000
```

Proceso grande (toda la hoja por lotes, post-compare sobre toda la hoja):

```bash
  python app.py --async --skip-drive --start-row 2 --max-concurrency 3 --batch-size 1000 --post-compare --compare-batch-size 1000
```

Report-only (regenerate daily report) / Solo informe (regenerar informe diario):

- Recalcula la comparaciÃ³n en toda la hoja y reescribe el informe, sin scrapear filas (Ãºtil para refrescar el informe del dÃ­a).

```bash
python app.py --async --skip-drive \
  --start-row 2 --limit 0 \
  --max-concurrency 1 --batch-size 1 \
  --post-compare --compare-batch-size 20000
```

## Compare statuses utility / Utilitario de comparaciÃ³n

Script to compare `STATUS DROPI` vs `STATUS TRACKING` and fill `COINCIDEN` and `ALERTA` using the same business rules as the updater.

- Normalization: both fields are normalized via `TrackerService.normalize_status()` before comparing.
- Coincidence: `COINCIDEN = TRUE` only if both normalized are non-empty and equal.
- Alert: computed via `TrackerService.compute_alert(dropi_norm, web_norm)`.
- Writes only when values change; sends batched updates to Sheets.

Usage:

```bash
python scripts/compare_statuses.py --start-row 2
python scripts/compare_statuses.py --start-row 500 --end-row 3000
python scripts/compare_statuses.py --start-row 2 --dry-run   # preview, no writes
```
