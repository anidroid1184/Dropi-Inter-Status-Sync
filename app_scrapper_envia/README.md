# App Scraper EnvÃ­a - EnvÃ­a Status Scraper via 17track.net

AplicaciÃ³n independiente para scraping de estados de tracking desde EnvÃ­a vÃ­a 17track.net.

## ğŸ“‹ Funcionalidad

- Scraping de estados desde portal web de 17track.net para EnvÃ­a
- Procesa **hasta 40 guÃ­as simultÃ¡neamente** en un solo batch
- Extrae el estado **crudo** exactamente como aparece en la web (sin normalizaciÃ³n)
- Actualiza solo la columna STATUS ENVIA con el texto crudo
- Soporte sÃ­ncrono y asÃ­ncrono con procesamiento por batches
- Procesamiento por rangos y batches
- Logging completo de operaciones

**Nota Importante**: El scraper guarda el estado tal cual aparece en la web, removiendo solo el tiempo (ej: "En trÃ¡nsito (2 DÃ­as)" -> "En trÃ¡nsito"). La normalizaciÃ³n y comparaciÃ³n se realiza en `app_comparer`.

## ğŸš€ InstalaciÃ³n

```bash
cd app_scrapper_envia
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
playwright install chromium
```

## âš™ï¸ ConfiguraciÃ³n

**IMPORTANTE: Esta app es completamente independiente**

1. Copiar `.env.example` a `.env` (en esta carpeta)
2. **Copiar tu `credentials.json` a esta carpeta `app_scrapper_envia/`**
3. Ajustar `SPREADSHEET_NAME` en `.env` para la hoja de EnvÃ­a

```bash
# Estructura requerida:
app_scrapper_envia/
â”œâ”€â”€ credentials.json     # â† TU ARCHIVO DE CREDENCIALES AQUÃ
â”œâ”€â”€ .env                 # â† TU CONFIGURACIÃ“N AQUÃ
â”œâ”€â”€ scraper_app.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ ...
```

## ğŸ“ Uso

```bash
# Scraping sÃ­ncrono bÃ¡sico
python scraper_app.py

# Scraping asÃ­ncrono con concurrencia (RECOMENDADO para grandes volÃºmenes)
python scraper_app.py --async --concurrency 3

# Solo procesar filas sin estado
python scraper_app.py --only-empty

# Modo simulaciÃ³n (sin escribir cambios)
python scraper_app.py --dry-run

# Procesar rango especÃ­fico
python scraper_app.py --start-row 2 --end-row 100

# Limitar cantidad de filas
python scraper_app.py --limit 50

# Batch size personalizado (por defecto: 40)
python scraper_app.py --async --batch-size 30
```

## ğŸŒ CÃ³mo Funciona

Este scraper utiliza el sitio 17track.net que permite rastrear envÃ­os de EnvÃ­a:

1. **URL**: `https://www.17track.net/es/carriers/envÃ­a-envia`
2. **MÃ©todo**: Ingresa hasta 40 guÃ­as en un textarea (una por lÃ­nea)
3. **Resultados**: Los resultados se cargan en la misma pÃ¡gina (no abre nueva pestaÃ±a)
4. **ExtracciÃ³n**:
   - ID de tracking desde: `<span title="014152617422" class="text-sm font-medium truncate">`
   - Status desde: `<div class="text-sm text-text-primary flex items-center gap-1">En trÃ¡nsito (2 DÃ­as)</div>`
   - El status se limpia para remover el tiempo: "En trÃ¡nsito (2 DÃ­as)" -> "En trÃ¡nsito"

## ğŸ“Š Columna Actualizada

- **STATUS TRANSPORTADORA**: Columna donde se guarda el estado crudo de la web

## ğŸ“Š Estructura de la Hoja

La hoja de Google Sheets debe tener las siguientes columnas:

- **A - ID DROPI**: ID interno de Dropi
- **B - ID TRACKING**: NÃºmero de guÃ­a de EnvÃ­a (se usa para scraping)
- **C - TRANSPORTADORA**: Nombre de la transportadora (debe contener "ENVIA")
- **D - STATUS DROPI**: Estado en Dropi
- **E - STATUS TRANSPORTADORA**: Estado de la transportadora (se actualiza por el scraper)
- **F - COINCIDEN**: Indicador de coincidencia de estados

## ğŸ”„ Flujo de Trabajo

1. Lee registros de Google Sheets
2. Filtra segÃºn criterios (rango, vacÃ­os, lÃ­mite)
3. Procesa en batches de hasta 40 guÃ­as
4. Extrae estados del portal 17track.net
5. Actualiza columna STATUS ENVIA con estados crudos
6. Log completo de operaciones

## ğŸ› ï¸ Arquitectura

- `scraper_app.py`: LÃ³gica principal y CLI
- `scraper_web.py`: Scraper sÃ­ncrono
- `scraper_web_async.py`: Scraper asÃ­ncrono con batches
- `scraper_sheets.py`: Cliente Google Sheets
- `scraper_config.py`: ConfiguraciÃ³n
- `scraper_logging.py`: Setup de logging
- `scraper_credentials.py`: Manejo de credenciales

## ğŸ” Ejemplos de Estados ExtraÃ­dos

```
"En trÃ¡nsito"
"Entregado"
"En proceso"
"Devuelto"
```

## ğŸ“ Notas

- El scraper NO normaliza estados - solo extrae el texto crudo
- La normalizaciÃ³n se realiza en `app_comparer`
- Procesa hasta 40 guÃ­as por batch para mÃ¡xima eficiencia
- Usa Playwright para navegaciÃ³n web robusta
- Maneja reintentos automÃ¡ticos en caso de errores
  python scraper_app.py --async --concurrency 5

# Procesar rango especÃ­fico

python scraper_app.py --start-row 100 --end-row 200

# Solo procesar filas sin estado

python scraper_app.py --only-empty

# Modo dry-run (simulaciÃ³n)

python scraper_app.py --dry-run --limit 10

```

## ğŸ“¦ Dependencias

- playwright: Web scraping
- gspread: Google Sheets API
- oauth2client: AutenticaciÃ³n Google
- python-dotenv: ConfiguraciÃ³n

## ğŸ”§ ParÃ¡metros

- `--start-row`: Fila inicial (default: 2)
- `--end-row`: Fila final (default: todas)
- `--limit`: LÃ­mite de filas
- `--async`: Usar scraper asÃ­ncrono
- `--concurrency`: PÃ¡ginas concurrentes (default: 3)
- `--batch-size`: TamaÃ±o de batch (default: 5000)
- `--only-empty`: Solo filas sin estado
- `--dry-run`: Simular sin escribir
```
